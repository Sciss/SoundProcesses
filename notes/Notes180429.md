# Towards better server timing

There are two fundamental options:

- rely on OSC bundle time and thus accept the jitter between CPU and audio clock.
- in the face no audio clock implementation materialising in the foreseeable future,
  implement our own servo mechanism
  
The first obviously is simpler. We would distinguish between an "immediate" `cursor.step`
and a scheduled `cursor.step` which by its nature cannot contain user initiated interaction.
This latter transaction could set the bundle time, giving some `AuralSystem` or `AuralContext` based
latency.

The second is more involved. Are there third party UGen plugins we could use to send precise
clock information back to the client? Obviously, we could use a `Phasor`, or `Timer` etc. but they
will be prone to floating point noise. Down to the size of one block, actions such as `s_new` could
be deferred by starting a paused synth and a 'resuming' node based on audio clock. But can we do the
same for the other commands such as `n_move` or `n_set`? What is the state of supernova, which is
claimed to support audio-clock? It may well be not worth the effort to implement a complex work-around
if there are approaches that bring us already much closer to the target.

(Having a look at sc3-plugins, it seems the audio clock experiments did not make it into that).
Perhaps a UGen that uses `u_cmd` to schedule a bundle, using audio clock instead, would be the easiest
approach without forking scsynth?

--------

It appears we should try to implement the bundle times approach first; because once we have experience with
this, it would be easier to reactivate an effort for audio clock mode for scsynth, as we have a system that
can readily be tested.

## Transaction order

This would create the problem that an interactive 'immediate' transaction following a scheduled transaction
could send commands to the sound server that are 'too early'. The simplest way to handle this would be
to adapt `Resource#timeStamp` to reflect the bundle-scheduling of the resource, and propagate messages from
a transaction relying on such resources to use at least the same time stamp. Without further mechanism, this
would, however, essentially disallow immediate bundles after a while, as everything will at some point
rely on the scheduled bundles. The additional mechanism would be a sort of acknowledgement of `synced` that
clears the bundle time as having-been-executed. Alternatively we disable "late" warning messages, and instead of
immediate bundles, always use system clock times (with zero latency if there are no dependencies).

Theoretically, this would already be handled by `ServerImpl`'s way of checking `bundleReplySeen`?

## Starting schedulers

Special care needs to be taken of a `cursor.step` meant to run `Transport#play`, as this of course should also
use scheduled bundles.

## Analysis of current mechanism

Theoretically a transaction can produce multiple bundles through `addMessage`: This happens if an asynchronous
message has dependencies on a synchronous message. This is a rather odd case which almost certainly is
unintended, but nevertheless needs to be handled.

`Txn.Bundle` would have to be extended by a bundle time or latency indicator. In the unlikely case of several
bundles per txn, only the first could have latency, the others would automatically degenerate to 'immediate'
execution.

Where is latency indicated? These are the possibilities:

- `lucre.synth.Txn` is extended by a flag that could be set at any point during the transaction.
- `stm.Cursor` is extended or sub-classed so there is a variant of `step` that indicates the scheduled status.

The second would be cleaner, because the first would anyway imply that the scheduler is the point where that
flag is set, and it happens right after calling `cursor.step`. The cost of an `stm.Cursor` sub-class is that
the new type will bubble up everywhere from `Scheduler` upwards. The disadvantage of extending the type is that
we have to create new major versions up from Lucre. Also, if we want to include a possible to _read_ that flag
from the resulting transaction, we can't have `PlainTxn` be an `object`.

## Configuration

Should there be only a simple flag, or an entire configuration for a transaction? For instance, several
`Transport` instances might opt for different latencies? Or, we could introduce multi-user support this way?
Also note that currently, logical transaction time is merely available from the `Scheduler`, not the transaction
(and thus not `ServerImpl`). The scheduler remains a time-zero based on system nano-time, and a txn-local for
freezing the time offset. It would have to be augmented to also store the nano-time in the txn-local, and/or
determine that at the point it runs `cursor.step`, then pass it into the new variant of `step`.

Consequently, rolled back and retried transactions could be a problem, since it means the bundle times would
also move forward? _Not true_. So `eventReached` sets the txn-local to yield precise sample frames; it would be
here that it also calculates a logical system clock? Or rather, that clock could always be calculated based on
`timeZero` and the logical frames advanced. This should give us jitter no larger than scsynth's block-size.

The flag/schedule logical system time could then be read from `lucre.synth.Txn` by `ServerImpl` in `send`.

All this seems to be fairly independent of whether we change, at a later point, to an audio clock mechanism.

## Bundle times

With `Bundle.millis` there is a resolution problem. For example, at 48 kHz sampling rate, one millisecond
corresponds with 48 sample frames. Even in the absence of CPU time / audio time correspondence, it would be
good to have more fine grained bundle times. The OSC time-tag supports much higher resolution (32 bits for
one second). We can use `Timetag(raw)` to construct more precise tags. Given the magnitude of the latency,
it is probably sufficient to capture both `currentTimeMillis` and `nanoTime` at the creation of scheduler,
i.e. we introduce a constant penalty of 1ms latency to the entire system, and logical time for the transaction
could be computed from zero-time-millis plus logical sample frames elapsed. The server-impl would add its
latency to the bundle time, if that time corresponded to zero-latency transaction time.

So what would the new cursor method look like?

```scala
import de.sciss.lucre.stm.Sys

trait Cursor[S <: Sys[S]] {
  def stepAt[A](millis: Long, nanos: Int)(fun: S#Tx => A): A
}
```

Another option would be to pass in OSC time-tag resolution directly (should be simply to do
arithmetic with it?) Time-tag has 32 bit for seconds since 1900, so there will be a problem in 2036.
Since anyway we won't be better than nano-seconds resolution, we might as well use the millis since 1970,
multiply them by 1000000 (roughly 20 bits) and add nano-delta. Then we can easily add the latency, and relatively
easy convert to time-tag raw value. `TimeRef.SampleRate` has a resolution of roughly 70ns.

```scala
import de.sciss.lucre.stm.Sys

trait Cursor[S <: Sys[S]] {
  def stepAt[A](nanosSince1970: Long)(fun: S#Tx => A): A
}
```
